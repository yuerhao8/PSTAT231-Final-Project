---
title: "PSTAT131_FinalProject Credit Card Fraud"
author: "Yuer Hao"
output:
  html_document:
    toc: yes
    code_folding: hide
    toc_depth: 2
    toc_float: yes
    number_sections: no
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.pos = 'H')
library(knitr)
```

\    
“Have you gone insane? How come to earth you were doing the a Casino?!" My sweet dream was interrupt by the call (in fact yelling) of my mother. Of course I did not spent 10,000 yuan in a casino--it was credit card fraud. Studies shows nearly half of the adult population in America (127 million) has suffered a fraudulent transaction using one of their credit or debit cards. Especially in card use overseas, credit card information is easily attainable as time differentiates in the banking systems. Although most banks usually discharge the cardholder, the money are not refunded in many cases. So, how can we use our acknowledge from PSTAT231 to research and analysis this real world problem?  
```{r, fig.align='center', fig.show='hold'}
knitr::include_graphics("/Users/Yuer_Hao/Desktop/PSTAT231-Final-Project/Picture/Cover.jpg")
```

## 1. Introduction
#### 1.1 The purpose of this project 
Every year, fraudulent transactions with credit cards result in billions of dollars in losses. The key to minimizing these losses is the development of effective fraud detection algorithms, and increasingly, these algorithms depend on cutting-edge machine learning methods to help fraud investigators. Nevertheless, because of the non-stationary distributions of the data, the extremely unbalanced classification distributions, and the ongoing streams of transactions, designing fraud detection algorithms is especially difficult. Due to confidentiality concerns, publicly available information are also hard to come by, leaving many questions regarding how to approach this problem in the dark.  

#### 1.2 Some facts you need to know about Credit Card Fraud  

- A total of 127 million adults in America—or nearly half of the population—have experienced a fraudulent transaction on one of their credit or debit cards. Card fraud has happened more than once to more than one in three people who use credit or debit cards.

- On American credit and debit cards, the typical charge was $62, which translates to around 8 billion in attempted fraudulent transactions. Only around 40% of cardholders have email or text notifications from their bank or credit card issuer activated.

- Only 19% of victims with alerts turned on had to take further action to reverse fraudulent charges, compared to about 81 percent of victims without these warnings.

```{r video, fig.align='center', fig.show='hold', message = FALSE}
library(vembedr)
embed_youtube("c-DxF1XVATw")
embed_youtube("2xBddrmbG7w")
```
#### 1.3 Why might this model be useful?
The model with the highest predicted fraud detection performances on the following block of transactions is the optimal model for a fraud detection system.

## 2. Dataset Overview
This project uses MACHINE LEARNING GROUP - ULB's dataset from [Kaggle](<https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud>)  

The dataset contains credit card transactions done by European cardholders in September 2013.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is quite unbalanced, with frauds making up 0.172% of all transactions in the positive class (frauds) account.    
There are 284807 observations and 31 columns in this dataset. There are 1 response variable and 30 predictor variables. Additionally, 30 of them are numerical, while 1 is binary. The response variable, "Class," has a value of 1 in cases of fraud and 0 in all other cases.

- `Time`: (Data Type: continuous) Number of seconds elapsed between this transaction and the first transaction in the dataset  

- `V1-V28`: (Data Type: continuous) May be result of a PCA Dimensionality reduction to protect user identities and sensitive features  

- `Amount`: (Data Type: continuous) Transaction Amount  

- `Class`: (Data Type: nominal) The response variable and it takes value 1 in case of fraud and 0 otherwise  
*Note: a full copy of the codebook is available in zipped final projecct files.*

### 2.1 Loading Data and Packages
```{r Loading packages, warning=FALSE, include=FALSE}
library(tidymodels)
library(tidyverse)
library(ISLR) # For the Smarket data set
library(ISLR2) # For the Bikeshare data set
library(discrim)
library(poissonreg)
library(vip)
library(janitor)
library(ROSE)
library(randomForest)
library(xgboost)
library(kknn)
library(skimr)
library(corrr)
library(klaR) # for naive bayes
library(forcats)
library(corrplot)
library(pROC)
library(recipes)
library(rsample)
library(parsnip)
library(workflows)
library(janitor)
library(glmnet)
library(rpart.plot)
library(patchwork)
library(janitor)
library(skimr)
tidymodels_prefer()
set.seed(231)
```
```{r Loading Data, , fig.align='center', fig.show='hold'}
#read, explore, check nulls, distributions
raw_data <- read.csv("/Users/Yuer_Hao/Desktop/PSTAT231 Final Project/data/creditcard.csv")
head(raw_data)
skim(raw_data)%>%
dplyr:: select(skim_type, skim_variable, n_missing, numeric.mean, numeric.hist)
```
## 3. Data Cleaning  
While the data set that was downloaded was tidy, a few different cleaning steps were necessary before the split occurred: 

### 3.1 Clean name 
```{r column name,class.source = 'fold-show'}
raw_creditc <- raw_data %>% 
  clean_names()
```
### 3.2 Deal with imbalanced problems  
Let's now determine whether or not our response variable is balanced. If not, we must resolve the situation.  
```{r balance check,class.source = 'fold-show'}
table(raw_creditc$class)
```
We can tell our response variable is highly unbalanced. Observations on "0" class are far more frequent than "1" class. We need to use some functions to address this problem, otherwise this will have a significant impact on our prediction models. The TA suggests that we employ the ovun.sample() function to processing it.
```{r}
creditc<- ovun.sample(class~.,data = raw_creditc,
                             p=0.5,seed = 1,method = "under")$data
```

```{r,class.source = 'fold-show'}
table(creditc$class)
```
our response variable is almost balanced.   

### 3.3 Convert class to factor
```{r, class.source = 'fold-show'}
creditc <- creditc %>%
  mutate(class = factor(class, levels = c("1", "0"))) 
```

### 3.4 Summary 
```{r, class.source = 'fold-show'}
summary(creditc$amount)
var(creditc$amount)
# show how many observations and variables in the new dataset
dim(creditc) 
```


### 3.5 Clean name

```{r, class.source = 'fold-show'}
creditc$amount <- scale(creditc$amount)
head(creditc)
```
We completed the the process of data cleaning.

## 4. Data Split

The data was stratified sampling by *class* , and spitted to 70% training set and 30% testing set.

```{r, class.source = 'fold-show'}
set.seed(2022)
creditc_split <- initial_split(creditc, prop = 0.70, strata = class)
creditc_train <- training(creditc_split)
creditc_test <- testing(creditc_split)

# check dimension
dim(creditc_train)
dim(creditc_test)
```

- The training data has 675 observations.  
- The testing data has 291 observations.

## 5. Data Exploration

### 5.1 Bar Plot and Table  
\
After employing the ovun.sample() function to processing the data, we can see that the number of card fraud is balanced from the table and plot.

```{r, class.source = 'fold-show', fig.align='center', fig.show='hold'}
table(creditc_train$class)
```

```{r,fig.cap="The Count of Fraud in the Training Dataset", fig.align='center', fig.show='hold'}
creditc_train %>% 
  ggplot(aes(x = class,fill=class)) +
  geom_bar() +
  ggtitle("Count of Fraud") +
  labs(y = "Count", x = "Transaction Type") +
  scale_fill_brewer(labels = c("Count of fraud", "Count of Other Transaction"))
```


### 5.2 Correlation Matrix  
Since the majority of the variables were subjected to a Principal Component Analysis (PCA) algorithm, we have seen that they are not all correlated. As a result, we are uncertain if the relevance of the Principal Components is reflected in the order in which the variables are numbered.  

```{r,fig.cap="The Correlation Matrix of the Training Dataset", warning=FALSE, fig.align='center', fig.show='hold'}
creditc_train %>% 
  select(is.numeric) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(method = "color",
           type = "full", 
           addCoef.col = "black", 
           order = "hclust", 
           tl.cex = 0.45,
           number.cex = 0.35)
```
As we would anticipate following the feature transformation, there are relatively few associated variables visible from the correlation matrix shown. Time and Amount, the two significant aspects, exhibit some relative connection with some variables. It would be difficult to conclusively suggest a link between any of them given their low levels. Additionally, it suggests that the likelihood of any linearity in our dataset would be quite minimal.

### 5.3 Target Variable Data Transformation and Analysis
Given their transformation and standardization, we have no information of the numerical predictors for confidentiality, with the exception of Amount & Time. Amount therefore emerged as maybe the most illuminating for the feature variable analysis. We have to log-scale the variable in order to better comprehend its distribution.
```{r,fig.cap="Target Variable `amount` Analysis", fig.align='center', fig.show='hold'}
## Target Variable `amount` Analysis
creditc_train$amount %>% summary()

creditc_train %>% ggplot(aes(amount)) +
  geom_histogram(bins=30) +
  scale_x_log10() +
  labs(
  x = "Dollar Amount (Log Scale)", y = "Frequency (Count)",
  title= "Distribution of Transaction Amount (Log Scaled)"
 )
```
There will be minimal probability for any outliers among the data values for V1, V2,..., V28 since the majority of predictors have been modified. Therefore, as Amount is the only useful numerical attribute, we shall just look at it.

```{r,fig.cap="Distribution of Transaction Amount", fig.align='center', fig.show='hold'}
creditc_train %>% ggplot(aes(x=amount)) +
  geom_boxplot() +
  labs(x = "Amount ($USD)", title= "Distribution of Transaction Amount")
```
We can see a significant number of outliers on the higher end of the distribution from the boxplot above. It would signify transactions involving large amounts of money in thousands. When developing the predictive models, we would consider how this skewed distribution might affect data transformation or the choice of models that are resistant to such feature types.    

In order to analysis the variable time, we will examine transaction time to look for any abnormalities. We will create a scatterplot using only the fraud dataset.

```{r,fig.cap="Target Variable `time` Analysis", fig.align='center', fig.show='hold'}
## Target Variable `time` Analysis
## Are there any tendency in time where fraud occurred?
# Splitting data by fraud class
CC_no_fraud <- creditc_train %>% filter(class == 0)
CC_fraud <- creditc_train %>% filter(class == 1)
# Scatterplot

CC_fraud %>% ggplot(aes(x=time, y=amount)) +
  geom_point() +
  labs(
  y = "Amount ($)", 
  x = "Time (s)",
  title= "Fraudulent Transactions Across Time"
 )
```
There doesn't seem to be a clustering structure on a timespan in the graph above. Therefore, we would suppose that fraud happened relatively randomly throughout time.  


## 6.Model fitting  

The goal of model selection is to choose the model that will produce the best predictions on upcoming data. The model with the highest predicted fraud detection performances on the following block of transactions is the optimal model for a fraud detection system.

### 6.1 Create Recipe

```{r, class.source = 'fold-show'}
creditc_recipe <- recipe(class ~ ., creditc_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

## Model 1: Logistic Regression, LDA/QDA

For Model 1, I will fitting 3 different models and find the best one with highest accuracy.

### M1.1a) Logistic Regression

```{r, class.source = 'fold-show'}
# classification using the *glm* engine.
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(creditc_recipe)

log_fit <- fit(log_wkflow, creditc_train)
```

### M1.1b) LDA - Linear Discriminant Analysis

```{r, class.source = 'fold-show'}
# classification using the *MASS* engine
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(creditc_recipe)

lda_fit <- fit(lda_wkflow, creditc_train)

```

### M1.1c) QDA - Quadratic Discriminant Analysis

```{r, class.source = 'fold-show'}
# classification using the *MASS* engine
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(creditc_recipe)

qda_fit <- fit(qda_wkflow, creditc_train)
```

### M1.2 Comparing three models

```{r, fig.align='center', fig.show='hold'}
log_acc <- predict(log_fit, 
                   new_data = creditc_train, 
                   type = "class") %>% 
  bind_cols(creditc_train %>% select(class)) %>% 
  accuracy(truth = class, estimate = .pred_class)

lda_acc <- predict(lda_fit, 
                   new_data = creditc_train, 
                   type = "class") %>% 
  bind_cols(creditc_train %>% select(class)) %>% 
  accuracy(truth = class, estimate = .pred_class)

qda_acc <- predict(qda_fit, 
                   new_data = creditc_train, 
                   type = "class") %>% 
  bind_cols(creditc_train %>% select(class)) %>% 
  accuracy(truth = class, estimate = .pred_class)

result_tab <- bind_rows(log_acc, lda_acc, qda_acc) %>% 
  tibble() %>% mutate(model = c("Logistic", "LDA", "QDA")) %>% 
  select(model, .estimate) %>% 
  arrange(-.estimate)

result_tab
```

I'm going to test the Logistic Regression model to the testing data since it has the highest training estimation with 0.9437037.

### M1.3 Fitting testing data

```{r, warning=FALSE}
log_test <- fit(log_wkflow, creditc_test)
predict(log_test, new_data = creditc_test, type = "class") %>% 
  bind_cols(creditc_test %>% select(class)) %>% 
  accuracy(truth = class, estimate = .pred_class)
```
Based on the table, we can see that the Logistic Regression model did a great prediction with 0.9896907 accuracy.

### M1.4 Confusion matrix and ROC

Then, we can check the model by using visualization:

#### Confusion Matrix

```{r ,fig.cap="Model 1 - Confusion Matrix", fig.align='center', fig.show='hold'}
augment(log_test, new_data = creditc_test) %>%
  conf_mat(truth = class, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

\

#### ROC Check

```{r,fig.cap="Model 1 - ROC Check" , fig.align='center', fig.show='hold'}
augment(log_test, new_data = creditc_test) %>%
  roc_curve(class, .pred_1) %>%
  autoplot()
```

```{r,class.source = 'fold-show'}
# Calculate AUC
augment(log_test, new_data = creditc_test) %>%
  roc_auc(class, .pred_1)
```

\
The reliability of our model is also confirmed by the confusion matrix, 0.9896907 accuracy, and 0.9948734 ROC_AUC solid performance. There are 288 of the 291 observations in the matrix were correctly predicted by the Logistic Regression, and the curve is virtually at the left-top corner.

## Model 2: Decision tree

For the second model, I would like to set up a decision tree. 

### M2.1 Set up and `rpart.plot()`

```{r, class.source = 'fold-show', fig.align='center', fig.show='hold'}
# set up model and workflow
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_fit <- class_tree_spec %>%
  fit(class ~ ., data = creditc_train)

class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint=FALSE)
```

\
### M2.2 Fit decision tree

```{r}
augment(class_tree_fit, new_data = creditc_test) %>%
  accuracy(truth = class, estimate = .pred_class)
```

### M2.3 Confusion matrix

Let us take a look at the confusion matrix:

```{r, fig.align='center', fig.show='hold'}
augment(class_tree_fit, new_data = creditc_test) %>%
  conf_mat(truth = class, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

```{r,class.source = 'fold-show'}
# Calculate AUC
augment(class_tree_fit, new_data = creditc_test) %>%
  roc_auc(class, .pred_1)
```

\
We can see decision tree have successful predicted 265 of 291 observations from the matrix with 0.9106529	 accuracy and 0.9242818 ROC_AUC.


## Model 3: Nearest Neighbors

The Nearest Neighbor model is then applied. Folding the training data is where we start. Utilize k-fold cross-validation with k=5.

```{r, class.source = 'fold-show'}
creditc_fold <- vfold_cv(creditc_train, v = 5, strata = class)
```

### M3.1 Set up

```{r, class.source = 'fold-show', warning=FALSE}
knn_model <- nearest_neighbor(neighbors = tune(),
            mode = "classification") %>% 
            set_engine("kknn")

knn_workflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(creditc_recipe)

# set-up tuning grid 
knn_params <- parameters(knn_model)

# define grid
knn_grid <- grid_regular(knn_params, levels = 2)
```

### M3.2 Tune the model

```{r, class.source = 'fold-show'}
knn_tune <- knn_workflow %>% 
  tune_grid(resamples = creditc_fold, 
            grid = knn_grid)
```

```{r, class.source = 'fold-show'}
arrange(collect_metrics(knn_tune),desc(mean))
```

### M3.3 Fit the nearest model

We using the best parameter to fit the model.

```{r,class.source = 'fold-show'}
best_comp <- select_best(knn_tune, metric = "roc_auc")
creditc_final <- finalize_workflow(knn_workflow, best_comp)
knn_fit <- fit(creditc_final,data = creditc_train)

augment(knn_fit, new_data = creditc_test) %>%
  accuracy(truth = class, estimate = .pred_class)
```

### M3.4 Heat map

We can use the heat map to clearly see the prediction.

```{r, fig.align='center', fig.show='hold'}
augment(knn_fit, new_data = creditc_test) %>%
  conf_mat(truth = class, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

### M3.5 AUC

```{r,class.source = 'fold-show'}
# Calculate AUC
augment(knn_fit, new_data = creditc_test) %>%
  roc_auc(class, .pred_1)
```
We can see the Nearest Neighbors have 0.8797251 accuracy and high ROC_AUC with 0.9323143 and have successful predicted 256 of 291 observations from the matrix.

## Model 4: Random forest

Next, I'm going to set up a random forest model and workflow.\

### M4.1 Set up

```{r, class.source = 'fold-show'}
rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(), 
                       min_n = tune()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_spec) %>% 
  add_recipe(creditc_recipe)

param_grid_rf <- grid_regular(mtry(range = c(1, 29)), 
# since we only have 29 predictors(V1-V28), the range of mtry should not be smaller than 1 or larger than 29.
                           trees(range = c(10, 100)), 
# Due to we have a large size of the dataset, 100 trees as maximum should be a good choise
                           min_n(range = c(1, 4)),
                           levels = 2)
```

### M4.2 Tune the model and print an `autoplot()` of the results.

```{r}
tune_res <- tune_grid(
  rf_wf, 
  resamples = creditc_fold, 
  grid = param_grid_rf, 
  metrics = metric_set(roc_auc)
)
# print result
autoplot(tune_res)
```

```{r}
arrange(collect_metrics(tune_res),desc(mean))
```

Base on the plots and table, we can tell that the more trees we add the better performance we get.

### M4.3 Important plot

By using `vip()`function, we can create a variable importance plot with the best-performing random forest model fit on the training set.

```{r, fig.align='center', fig.show='hold'}
best_complexity <- select_best(tune_res, metric = "roc_auc")
creditc_final <- finalize_workflow(rf_wf, best_complexity)
rf_fit <- fit(creditc_final,data = creditc_train)
rf_fit %>%
  extract_fit_engine() %>%
  vip()
```

\
It is clear that variable v14 is the most important one. All variables, nevertheless, are significant in this model.\

### M4.4 Fit random forest model

```{r}
augment(rf_fit, new_data = creditc_test) %>%
  accuracy(truth = class, estimate = .pred_class)
```

The Random forest model has 0.9175258 accuracy.

### M4.5 Heat map

```{r, fig.align='center', fig.show='hold'}
augment(rf_fit, new_data = creditc_test) %>%
  conf_mat(truth = class, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

### M4.6 AUC

```{r,class.source = 'fold-show'}
# Calculate AUC
augment(rf_fit, new_data = creditc_test) %>%
  roc_auc(class, .pred_1)
```
With a 0.9175258 accuracy and ROC_AUC of 0.9555141, the decision tree successfully predicted 267 out of 291 observations from the matrix.  

## Model 5: Boost tree

At last, I will set up a boost tree model.

### M5.1 Set up

```{r, class.source = 'fold-show', fig.align='center', fig.show='hold'}
boost_spec <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_grid <- grid_regular(trees(c(10,200)),levels = 10)

boost_wf <- workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(creditc_recipe)

boost_tune_res <- tune_grid(
  boost_wf, 
  resamples = creditc_fold, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc),
)
 
autoplot(boost_tune_res)
```

\
The roc_auc keep increasing and reach the peak around 0.9847 with 30 trees.

### M5.2 Select best tree

```{r, class.source = 'fold-show'}
best_boost <- select_best(boost_tune_res)
boost_final <- finalize_workflow(boost_wf, best_boost)
boost_final_fit <- fit(boost_final, data = creditc_train)
```

### M5.3 Fit tree

```{r}
augment(boost_final_fit, new_data = creditc_test)%>%
  accuracy(truth = class, estimate = .pred_class)
```

### M5.4 Heat map

```{r, fig.align='center', fig.show='hold'}
augment(boost_final_fit, new_data = creditc_test) %>%
  conf_mat(truth = class, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```


### M5.5 ROC

```{r, fig.align='center', fig.show='hold'}
augment(boost_final_fit, new_data = creditc_test)%>%
  roc_curve(class, .pred_1) %>%
  autoplot()
```

### M5.6 AUC

```{r,class.source = 'fold-show'}
# Calculate AUC
augment(boost_final_fit, new_data = creditc_test) %>%
  roc_auc(class, .pred_1)
```
We can see the best boost tree have a 0.9072165 accuracy and 0.9722642 ROC_AUC. It successful predicted 264 of 291 observations from the matrix.  

### 7. Conclusion

In this project to detect credit card fraud, we aimed to demonstrate five different types of methods by analyzing transformed imbalanced dataset.   
The performance evaluation graph:  

| Model  | Accuracy  |   AUC     |
|:--------------|-----------|----------:|
| Logistic Regression| 0.9896907 | 0.9948734 | 
| Boosted Tree  | 0.9072165 |  0.9722642 | 
| Random Forest | 0.9175258 | 0.9555141 |  
| Decision Tree | 0.9106529 | 0.9242818 |  
| K Nearest Neighbor| 0.8797251 | 0.9323143 | 
 

For the purposes of this project, we train many prediction models to perform the same forecast job and then compare the results to decide the final "best" forecast model with the highest accuracy. Since we balanced the data before training the model, we can use both the confusion matrix accuracy and the accuracy using the Area Under the Precision-Recall Curve (AUC) to analysis our models' prediction. Although other models performed well, the Logistic Regression model yielded the highest accuracy of 0.9896907 with the 0.9948734 AUC. we will present the Logistic Regression as our final model.

### 8. Refrences
- MACHINE LEARNING GROUP - ULB's dataset from Kaggle https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
- https://www.security.org/digital-safety/credit-card-fraud-report/
- https://www.youtube.com/watch?v=2xBddrmbG7w
- https://www.youtube.com/watch?v=c-DxF1XVATw
- https://fraud-detection-handbook.github.io/fraud-detection-handbook/Chapter_References/shared_functions.html
